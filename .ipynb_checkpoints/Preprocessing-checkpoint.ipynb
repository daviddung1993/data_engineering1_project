{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f9cf3b",
   "metadata": {},
   "source": [
    "# Project Group 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb3c30",
   "metadata": {},
   "source": [
    "### Preprocessing, Null values and Unify dataframes, one for uber data and one for For-Hire Vehicle data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48834a48",
   "metadata": {},
   "source": [
    "### Setting up a Spark Session to work with structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66f36790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1969b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the code on local machine or on server\n",
    "server_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34bce2fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/16 00:15:01 WARN Utils: Your hostname, nanook resolves to a loopback address: 127.0.1.1; using 192.168.1.69 instead (on interface enp5s0)\n",
      "22/03/16 00:15:01 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/naeim/anaconda3/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/16 00:15:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "if server_mode:\n",
    "    #New API\n",
    "    spark_session = SparkSession\\\n",
    "            .builder\\\n",
    "            .master(\"spark://192.168.2.74:7077\") \\\n",
    "            .appName(\"Project_G1_Naeim\")\\\n",
    "            .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "            .config(\"spark.dynamicAllocation.shuffleTracking.enabled\",True)\\\n",
    "            .config(\"spark.shuffle.service.enabled\", False)\\\n",
    "            .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"300s\")\\\n",
    "            .config(\"spark.executor.cores\",1)\\\n",
    "            .config(\"spark.driver.port\",9998)\\\n",
    "            .config(\"spark.blockManager.port\",10005)\\\n",
    "            .getOrCreate()\n",
    "\n",
    "    # Old API (RDD)\n",
    "    spark_context = spark_session.sparkContext\n",
    "    spark_context.setLogLevel(\"ERROR\")\n",
    "    # spark_context.setLogLevel(\"INFO\")\n",
    "    \n",
    "else:\n",
    "    # local version, deactivate later!\n",
    "    spark_session = SparkSession.builder.appName('Project_G1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07711204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.69:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Project_G1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f7d809a4c70>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_session # to get some info about the Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f145c744",
   "metadata": {},
   "source": [
    "### Check the existing source files and their names in the remote directories before downloading them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "563f4dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files with csv extension: 19\n",
      "['taxi-zone-lookup.csv', 'uber-raw-data-apr14.csv', 'uber-raw-data-aug14.csv', 'uber-raw-data-jul14.csv', 'uber-raw-data-jun14.csv']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "\n",
    "urls = ['https://github.com/fivethirtyeight/uber-tlc-foil-response/blob/master/uber-trip-data/',\n",
    "        'https://github.com/fivethirtyeight/uber-tlc-foil-response/blob/master/other-FHV-data/',\n",
    "        'https://github.com/fivethirtyeight/uber-tlc-foil-response/']\n",
    "\n",
    "exts = ['csv'] #, 'xlsx'\n",
    "\n",
    "def get_content(url, ext=''):\n",
    "    page = requests.get(url).text\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    return [url + '/' + node.get('href') for node in soup.find_all('a') if node.get('href').endswith(ext)]\n",
    "file_names = []\n",
    "\n",
    "for url in urls:\n",
    "    for ext in exts:\n",
    "        for file in get_content(url, ext):\n",
    "            file_names.append(os.path.basename(file))\n",
    "\n",
    "print(f'Number of files with csv extension: {len(file_names)}')\n",
    "print(file_names[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07153807",
   "metadata": {},
   "source": [
    "### Download source files in local file system before working with Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3449c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository is ALREADY downloaded!\n"
     ]
    }
   ],
   "source": [
    "# !pip install gitpython\n",
    "from git import Repo\n",
    "if not os.path.isdir('DATA') or len(os.listdir('DATA')) == 0:\n",
    "    git_url = 'https://github.com/fivethirtyeight/uber-tlc-foil-response.git'\n",
    "    Repo.clone_from(git_url, 'Git_repo');\n",
    "    print('The repository is downloaded!')\n",
    "else:\n",
    "    print('The repository is ALREADY downloaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbf18dc",
   "metadata": {},
   "source": [
    "### Keeping useful files from git repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d38724e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['csv'] files already extracted!\n",
      "Number of new ['csv'] extracted files: 19\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "src = r'Git_repo'\n",
    "if os.path.isdir('Git_repo'):\n",
    "    os.system(\"rm -rf DATA\")\n",
    "    os.makedirs('DATA')\n",
    "    print(\"Old DATA directory deleted!\")\n",
    "    dest = r'DATA'\n",
    "\n",
    "    for path, subdirs, files in os.walk(src):\n",
    "        if '.git' in subdirs:\n",
    "            subdirs.remove('.git')\n",
    "        for name in files:\n",
    "            if name.endswith('.csv'): # or name.endswith('.xlsx'):\n",
    "                filename = os.path.join(path, name)\n",
    "                shutil.copy2(filename, dest)\n",
    "else:\n",
    "    print(f\"{exts} files already extracted!\")\n",
    "\n",
    "if os.path.isdir('DATA'):\n",
    "    path = os.getcwd() + '/DATA'\n",
    "    list_dir = os.listdir(path)\n",
    "    count = 0\n",
    "    for file in list_dir:\n",
    "        if file.endswith(exts[0]):\n",
    "            count += 1\n",
    "    print(f\"Number of new {exts} extracted files: {count}\")\n",
    "os.system(\"rm -rf Git_repo\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380e56f",
   "metadata": {},
   "source": [
    "### Load the CSV files from source folder, and call show() to verify the data is loaded to RAM correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a1b3c08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Laodig data in Spark & writing in HDFS\n",
    "df_list = {}\n",
    "for file in file_names:\n",
    "    # spark_file = 'df_'+ (os.path.splitext(file)[0]).split('-')[-1]\n",
    "    # df_list[file] = spark_session.read.csv('DATA/'+file, header=True, inferSchema=True)\n",
    "    df_list[file] = spark_session.read.option(\"header\",\"true\").csv('DATA/'+file)\n",
    "\n",
    "# df_list.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9c8b251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark dataframe exists\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql\n",
    "if df_list[file_names[0]] is not None and \\\n",
    "                isinstance(df_list[file_names[0]], \\\n",
    "                    pyspark.sql.dataframe.DataFrame):\n",
    "    print(\"Spark dataframe exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08e03dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('taxi-zone-lookup.csv', pyspark.sql.dataframe.DataFrame)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names[0], type(df_list[file_names[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ede443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+\n",
      "|LocationID|Borough|          Zone|\n",
      "+----------+-------+--------------+\n",
      "|         1|    EWR|Newark Airport|\n",
      "|         2| Queens|   Jamaica Bay|\n",
      "+----------+-------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list[file_names[0]].show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ff380a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: string (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list[file_names[0]].printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8321d34",
   "metadata": {},
   "source": [
    "### Check missing or Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a694e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code is inspired by 'pault' @ stackoverflow\n",
    "# Pyspark - Calculate number of null values in each dataframe column\n",
    "\n",
    "import pyspark.sql.functions as func\n",
    "from functools import reduce\n",
    "\n",
    "def count_null_values_in_df(any_df):\n",
    "    \n",
    "    df_agg = any_df.agg(*[func.count(func.when(func.isnull(c), c)).alias(c) for c in any_df.columns])\n",
    "    null_table = reduce(lambda a, b: a.union(b),(\n",
    "        df_agg.select(func.lit(c).alias(\"Column\"), func.col(c).alias(\"Nbr of Null\")) \n",
    "        for c in df_agg.columns))\n",
    "    \n",
    "    null_table.show()\n",
    "    print(f'Initial number of rows: {any_df.count()}')\n",
    "    print(\"************\\n\")\n",
    "    \n",
    "    any_df = any_df.na.drop(how='all') # to drop any row with all values as Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ffaa8b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = 1\n",
    "for key in df_list.keys():\n",
    "    if 'uber-raw-data' in key:\n",
    "        print(f'{x}: {key}')\n",
    "        count_null_values_in_df(df_list[key])   \n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeab903",
   "metadata": {},
   "source": [
    "### Preparing Uber raw data\n",
    "Uber data (4.5 million Uber pickups in New York City from April to September 2014, and 14.3 million more Uber pickups from January to June 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eb5239a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date/Time', 'Lat', 'Lon', 'Base']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_names = df_list['uber-raw-data-apr14.csv'].schema.names\n",
    "col_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "402553b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 92:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---+----+\n",
      "|Date/Time|Lat|Lon|Base|\n",
      "+---------+---+---+----+\n",
      "+---------+---+---+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "sc = spark_session.sparkContext\n",
    "\n",
    "col_names = df_list['uber-raw-data-apr14.csv'].schema.names\n",
    "mySchema = StructType([StructField(c, StringType()) for c in col_names])\n",
    "uber_raw_data = SparkSession(sc).createDataFrame(data=[], schema=mySchema)\n",
    "uber_raw_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31056fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files combined: 6\n",
      "Total number of rows in unified dataframe: 4534327\n"
     ]
    }
   ],
   "source": [
    "manual_rows = 0\n",
    "number_of_files = 0\n",
    "for key in df_list.keys():\n",
    "    if 'uber-raw-data' in key:\n",
    "        uber_raw_data = uber_raw_data.union(df_list[key])\n",
    "        manual_rows += df_list[key].count()\n",
    "        number_of_files += 1\n",
    "\n",
    "print(f\"Total number of files combined: {number_of_files}\")\n",
    "print(f'Total number of rows in unified dataframe: {manual_rows}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f933cf0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:===============================================>       (24 + 4) / 28]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows match in the unified file!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if (uber_raw_data.count() == manual_rows):\n",
    "    print(\"The number of rows match in the unified file!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb389091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if server_mode:\n",
    "    path = 'hdfs://192.168.2.74:9000/user/ubuntu/uber-tlc-foil-response/uber-trip-data'\n",
    "else:\n",
    "    path = os.getcwd() + '/DATA/'\n",
    "\n",
    "uber_unified_file = path+\"uber_raw_data.parquet\"\n",
    "uber_raw_data.write.mode(\"overwrite\").parquet(uber_unified_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6306d5",
   "metadata": {},
   "source": [
    "### To check if downloaded files exist in DFS system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "942573a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
    "if server_mode:\n",
    "    path = 'hdfs://192.168.2.74:9000/user/ubuntu/uber-tlc-foil-response/uber-trip-data/'\n",
    "else:\n",
    "    path = os.getcwd() + '/DATA/'\n",
    "\n",
    "path = path + 'uber_raw_data.parquet'\n",
    "print(fs.exists(sc._jvm.org.apache.hadoop.fs.Path(path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798757e9",
   "metadata": {},
   "source": [
    "## Read the unified \"uber_raw_data\" file in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56d53678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date/Time: string (nullable = true)\n",
      " |-- Lat: string (nullable = true)\n",
      " |-- Lon: string (nullable = true)\n",
      " |-- Base: string (nullable = true)\n",
      "\n",
      "+----------------+-------+--------+------+\n",
      "|       Date/Time|    Lat|     Lon|  Base|\n",
      "+----------------+-------+--------+------+\n",
      "|9/1/2014 0:01:00|40.2201|-74.0021|B02512|\n",
      "|9/1/2014 0:01:00|  40.75|-74.0027|B02512|\n",
      "|9/1/2014 0:03:00|40.7559|-73.9864|B02512|\n",
      "+----------------+-------+--------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4534327"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_raw_data_read_test = spark_session.read.parquet(uber_unified_file)\n",
    "uber_raw_data_read_test.printSchema()\n",
    "uber_raw_data_read_test.show(3)\n",
    "uber_raw_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fae0a9e",
   "metadata": {},
   "source": [
    "### Preparing For-Hire Vehicle (FHV) data\n",
    "FHV companies (10 files of raw data on pickups from 10 FHV companies. The trip information varies by company, but can include day of trip, time of trip, pickup location, driver's for-hire license number, and vehicle's for-hire license number.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e080354",
   "metadata": {},
   "source": [
    "#### Drop Null columns manually for each dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c02e73f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = 1\n",
    "for key in df_list.keys():\n",
    "    if 'uber-raw-data' not in key:\n",
    "        print(f'{x}: {key}')\n",
    "        print(df_list[key].dtypes)\n",
    "        count_null_values_in_df(df_list[key]) \n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c60971a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|   PuFrom|\n",
      "+---------+\n",
      "|MANHATTAN|\n",
      "|MANHATTAN|\n",
      "|MANHATTAN|\n",
      "|MANHATTAN|\n",
      "|MANHATTAN|\n",
      "|MANHATTAN|\n",
      "|MANHATTAN|\n",
      "|MANHATTAN|\n",
      "|   QUEENS|\n",
      "|MANHATTAN|\n",
      "+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list['Dial7_B00887.csv'].select('PuFrom').show(10)\n",
    "df_list['Dial7_B00887.csv'] = df_list['Dial7_B00887.csv']\\\n",
    "                        .na.fill('UNKOWN')\n",
    "df_list['Dial7_B00887.csv'].filter(df_list['Dial7_B00887.csv']\\\n",
    "                        .PuFrom.isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "912a2b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+--------------------+----+----+----+\n",
      "|    DATE|       TIME|     PICK UP ADDRESS| _c3| _c4| _c5|\n",
      "+--------+-----------+--------------------+----+----+----+\n",
      "|7/1/2014|12:00:00 AM| 874 E 139th St M...|null|null|null|\n",
      "|7/1/2014|12:01:00 AM| 628 E 141st St M...|null|null|null|\n",
      "|7/1/2014|12:01:00 AM| 601 E 156th St S...|null|null|null|\n",
      "|7/1/2014|12:01:00 AM| 708 E 138th St M...|null|null|null|\n",
      "|7/1/2014|12:02:00 AM| 700 E 140th St M...|null|null|null|\n",
      "+--------+-----------+--------------------+----+----+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+-----------+--------------------+\n",
      "|    DATE|       TIME|     PICK UP ADDRESS|\n",
      "+--------+-----------+--------------------+\n",
      "|7/1/2014|12:00:00 AM| 874 E 139th St M...|\n",
      "|7/1/2014|12:01:00 AM| 628 E 141st St M...|\n",
      "+--------+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/16 00:16:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DATE, TIME, PICK UP ADDRESS, , , \n",
      " Schema: DATE, TIME, PICK UP ADDRESS, _c3, _c4, _c5\n",
      "Expected: _c3 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/American_B01362.csv\n"
     ]
    }
   ],
   "source": [
    "df_list['American_B01362.csv'].show(5)\n",
    "columns_to_drop = ['_c3','_c4','_c5']\n",
    "df_list['American_B01362.csv'] = \\\n",
    "    df_list['American_B01362.csv'].drop(*columns_to_drop)\n",
    "df_list['American_B01362.csv'].show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "672d18d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/16 00:16:57 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: time_of_trip, start_lat, start_lng, \n",
      " Schema: time_of_trip, start_lat, start_lng, _c3\n",
      "Expected: _c3 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/Lyft_B02510.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+---------+----+\n",
      "|   time_of_trip|start_lat|start_lng| _c3|\n",
      "+---------------+---------+---------+----+\n",
      "|  9/4/2014 9:51| 40.64705|-73.77988|null|\n",
      "|8/27/2014 21:13| 40.74916|-73.98373|null|\n",
      "+---------------+---------+---------+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+---------------+---------+---------+\n",
      "|   time_of_trip|start_lat|start_lng|\n",
      "+---------------+---------+---------+\n",
      "|  9/4/2014 9:51| 40.64705|-73.77988|\n",
      "|8/27/2014 21:13| 40.74916|-73.98373|\n",
      "+---------------+---------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list['Lyft_B02510.csv'].show(2)\n",
    "df_list['Lyft_B02510.csv'] = \\\n",
    "    df_list['Lyft_B02510.csv'].drop('_c3')\n",
    "df_list['Lyft_B02510.csv'].show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e799102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+--------------------+--------------------+----+----+\n",
      "|    Date|          Time|     Street_Address |         City_State | _c4| _c5|\n",
      "+--------+--------------+--------------------+--------------------+----+----+\n",
      "|7/1/2014|    20:27     |    622 THIRD AV ...|     M           ...|null|null|\n",
      "|7/1/2014|    21:04     |     E 77TH ST   ...|     M           ...|null|null|\n",
      "+--------+--------------+--------------------+--------------------+----+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "+--------+--------------+--------------------+--------------------+\n",
      "|    Date|          Time|     Street_Address |         City_State |\n",
      "+--------+--------------+--------------------+--------------------+\n",
      "|7/1/2014|    20:27     |    622 THIRD AV ...|     M           ...|\n",
      "|7/1/2014|    21:04     |     E 77TH ST   ...|     M           ...|\n",
      "+--------+--------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/16 00:16:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Date, Time,     Street_Address ,     City_State , , \n",
      " Schema: Date, Time,     Street_Address ,     City_State , _c4, _c5\n",
      "Expected: _c4 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/Skyline_B00111.csv\n"
     ]
    }
   ],
   "source": [
    "df_list['Skyline_B00111.csv'].show(2)\n",
    "df_list['Skyline_B00111.csv'] = \\\n",
    "    df_list['Skyline_B00111.csv'].drop(*columns_to_drop)\n",
    "df_list['Skyline_B00111.csv'].show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c54e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['_c0','_c3']\n",
    "df_list['other-FHV-data-jan-aug-2015.csv'] = \\\n",
    "    df_list['other-FHV-data-jan-aug-2015.csv'].drop(*columns_to_drop)\n",
    "df_list['other-FHV-data-jan-aug-2015.csv'] = \\\n",
    "    df_list['other-FHV-data-jan-aug-2015.csv'].na.drop(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40195b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+------------+---------------+------------------+\n",
      "|        _c1|      _c2|         _c4|            _c5|               _c6|\n",
      "+-----------+---------+------------+---------------+------------------+\n",
      "|Base Number|Base Name|Pick Up Date|Number of Trips|Number of Vehicles|\n",
      "+-----------+---------+------------+---------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/16 00:16:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , , , , \n",
      " Schema: _c1, _c2, _c4, _c5, _c6\n",
      "Expected: _c1 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/other-FHV-data-jan-aug-2015.csv\n"
     ]
    }
   ],
   "source": [
    "df_list['other-FHV-data-jan-aug-2015.csv'].show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c254ed23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/16 00:16:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , , , , \n",
      " Schema: _c1, _c2, _c4, _c5, _c6\n",
      "Expected: _c1 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/other-FHV-data-jan-aug-2015.csv\n",
      "22/03/16 00:16:59 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , , , , \n",
      " Schema: _c1, _c2, _c4, _c5, _c6\n",
      "Expected: _c1 but found: \n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/other-FHV-data-jan-aug-2015.csv\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+------------+---------------+------------------+\n",
      "|Base_Number|           Base_Name|Pick_Up_Date|Number_of_Trips|Number_of_Vehicles|\n",
      "+-----------+--------------------+------------+---------------+------------------+\n",
      "|     B00013|   LOVE CORP CAR INC|  01/01/2015|            26 |               17 |\n",
      "|     B00014| NY ONE CORP CAR INC|  01/01/2015|            45 |               24 |\n",
      "|     B00029|COMMUNITY CAR SVC...|  01/01/2015|           731 |               36 |\n",
      "|     B00053| CHARGE AND RIDE INC|  01/01/2015|            10 |                9 |\n",
      "|     B00095|LIBERTY CAR SERVI...|  01/01/2015|           814 |               62 |\n",
      "|     B00221|PROFESSIONAL CAR ...|  01/01/2015|           220 |               46 |\n",
      "|     B00227|PARK WEST EXEC. S...|  01/01/2015|            36 |               28 |\n",
      "|     B00248|YELLOWSTONE TRANS...|  01/01/2015|         1,137 |              106 |\n",
      "|     B00254|   XYZ TWO WAY RADIO|  01/01/2015|           236 |              103 |\n",
      "|     B00280|FLEET RADIO DISPA...|  01/01/2015|            47 |               29 |\n",
      "+-----------+--------------------+------------+---------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_list['other-FHV-data-jan-aug-2015.csv'] = \\\n",
    "        df_list['other-FHV-data-jan-aug-2015.csv'].withColumnRenamed(\"_c1\", \"Base_Number\")\\\n",
    "       .withColumnRenamed(\"_c2\", \"Base_Name\").withColumnRenamed(\"_c4\", \"Pick_Up_Date\")\\\n",
    "       .withColumnRenamed(\"_c5\", \"Number_of_Trips\").withColumnRenamed(\"_c6\", \"Number_of_Vehicles\")\n",
    "\n",
    "df_list['other-FHV-data-jan-aug-2015.csv'] = \\\n",
    "        SparkSession(sc).createDataFrame(df_list['other-FHV-data-jan-aug-2015.csv']\\\n",
    "                              .tail(df_list['other-FHV-data-jan-aug-2015.csv'].count()-1)\\\n",
    "                              ,df_list['other-FHV-data-jan-aug-2015.csv'].schema)\n",
    "\n",
    "df_list['other-FHV-data-jan-aug-2015.csv'].show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d839c1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list['Lyft_B02510.csv'] = df_list['Lyft_B02510.csv'].na.drop()\n",
    "df_list['Federal_02216.csv'] = df_list['Federal_02216.csv']\\\n",
    "                        .na.fill('UNKOWN')\n",
    "\n",
    "df_list['American_B01362.csv'] = \\\n",
    "        df_list['American_B01362.csv'].withColumnRenamed(\"PICK UP ADDRESS\", \"PICK_UP_ADDRESS\")\n",
    "df_list['Prestige_B01338.csv'] = \\\n",
    "        df_list['Prestige_B01338.csv'].withColumnRenamed(\"PICK UP ADDRESS\", \"PICK_UP_ADDRESS\")\n",
    "\n",
    "df_list['Firstclass_B01536.csv'] = \\\n",
    "        df_list['Firstclass_B01536.csv'].withColumnRenamed(\"PICK UP ADDRESS\", \"PICK_UP_ADDRESS\")\n",
    "df_list['Federal_02216.csv'] = \\\n",
    "        df_list['Federal_02216.csv'].withColumnRenamed(\"Routing Details\", \"Routing_Details\")\n",
    "\n",
    "df_list['Skyline_B00111.csv'] = \\\n",
    "        df_list['Skyline_B00111.csv'].withColumnRenamed(\"    Street_Address \", \"Street_Address\")\n",
    "df_list['Skyline_B00111.csv'] = \\\n",
    "        df_list['Skyline_B00111.csv'].withColumnRenamed(\"    City_State \", \"City_State\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d838d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = 1\n",
    "all_columns = []\n",
    "for key in df_list.keys():\n",
    "    if 'uber-raw-data' not in key:\n",
    "        print(f'{x}: {key}')\n",
    "        print(df_list[key].dtypes)\n",
    "        df_list[key].show(3)\n",
    "        count_null_values_in_df(df_list[key])\n",
    "        all_columns.append(df_list[key].schema.names)\n",
    "        x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b0d406cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A unique list of column names:\n",
      " {'locationid', 'start_lat', 'status', 'active_vehicles', 'base_no', 'pu_address2', 'base_number', 'time', 'pu_address', 'pick_up_date', 'time_of_trip', 'pufrom', 'state', 'pick_up_address', 'address', 'date', 'street', 'street_address', 'number_of_vehicles', 'city_state', 'routing_details', 'borough', 'zone', 'trips', 'base_name', 'do_address', 'dispatching_base_number', 'start_lng', 'pu_address5', 'number_of_trips', 'pu_adress'}\n"
     ]
    }
   ],
   "source": [
    "all_columns = [val for sublist in all_columns for val in sublist]\n",
    "all_columns = [x.strip().lower() for x in all_columns]\n",
    "all_columns = set(all_columns)\n",
    "print(f\"A unique list of column names:\\n {all_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250377a",
   "metadata": {},
   "source": [
    "### Save cleaned files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e590d519",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "if server_mode:\n",
    "    path = 'hdfs://192.168.2.74:9000/user/ubuntu/uber-tlc-foil-response/other-FHV-data'\n",
    "else:\n",
    "    path = os.getcwd() + '/DATA/'\n",
    "\n",
    "for key in df_list.keys():\n",
    "    if 'uber-raw-data' not in key:\n",
    "        file = os.path.splitext(key)[0]+\".parquet\"\n",
    "        print(file)\n",
    "        df_list[key].write.mode(\"overwrite\").parquet(path+file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16769e10",
   "metadata": {},
   "source": [
    "### Preparing a unified file for FHV questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "73faef36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------------+\n",
      "|DATE|TIME|PICK_UP_ADDRESS|\n",
      "+----+----+---------------+\n",
      "+----+----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col_names = df_list['American_B01362.csv'].schema.names\n",
    "mySchema = StructType([StructField(c, StringType()) for c in col_names])\n",
    "FHV_10_companies_data = SparkSession(sc).createDataFrame(data=[], schema=mySchema)\n",
    "FHV_10_companies_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9423148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91712"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.'American_B01362.csv'\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(df_list['American_B01362.csv'])\n",
    "df_list['American_B01362.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fd226a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256519"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. 'Carmel_B00256.csv'\n",
    "tmp = df_list['Carmel_B00256.csv'].drop('Base_No')\n",
    "tmp = tmp.withColumnRenamed(\"PU_Adress\", \"PICK_UP_ADDRESS\")\n",
    "df_list['Carmel_B00256.csv'] = tmp\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(tmp)\n",
    "df_list['Carmel_B00256.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e8ffc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "194992"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. 'Dial7_B00887.csv'\n",
    "columns_to_drop = ['State','PuFrom', 'Address']\n",
    "tmp = df_list['Dial7_B00887.csv'].drop(*columns_to_drop)\n",
    "tmp = tmp.withColumnRenamed(\"Street\", \"PICK_UP_ADDRESS\")\n",
    "df_list['Dial7_B00887.csv'] = tmp\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(tmp)\n",
    "df_list['Dial7_B00887.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e8a1384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98550"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. 'Diplo_B01196.csv'\n",
    "tmp = df_list['Diplo_B01196.csv'].withColumnRenamed(\"PU_Address\", \"PICK_UP_ADDRESS\")\n",
    "df_list['Diplo_B01196.csv'] = tmp\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(tmp)\n",
    "df_list['Diplo_B01196.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87338deb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. 'Federal_02216.csv'\n",
    "columns_to_drop = ['DO_Address','Routing_Details', 'Status', 'PU_Address2']\n",
    "tmp = df_list['Federal_02216.csv'].drop(*columns_to_drop)\n",
    "tmp = tmp.withColumnRenamed(\"PU_Address5\", \"PICK_UP_ADDRESS\")\n",
    "df_list['Federal_02216.csv'] = tmp\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(tmp)\n",
    "df_list['Federal_02216.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc91763a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166769"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6.'Firstclass_B01536.csv'\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(df_list['Firstclass_B01536.csv'])\n",
    "df_list['Firstclass_B01536.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90f57c8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151925"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7.'Highclass_B01717.csv'\n",
    "tmp = df_list['Highclass_B01717.csv'].withColumnRenamed(\"PU_Address\", \"PICK_UP_ADDRESS\")\n",
    "df_list['Highclass_B01717.csv'] = tmp\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(tmp)\n",
    "df_list['Highclass_B01717.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c06e8661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+---------+\n",
      "|   time_of_trip|start_lat|start_lng|\n",
      "+---------------+---------+---------+\n",
      "|  9/4/2014 9:51| 40.64705|-73.77988|\n",
      "|8/27/2014 21:13| 40.74916|-73.98373|\n",
      "| 9/4/2014 14:16| 40.64065|-73.97594|\n",
      "+---------------+---------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8.'Lyft_B02510.csv'\n",
    "df_list['Lyft_B02510.csv'].show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "807c7b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127696"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9.'Skyline_B00111.csv'\n",
    "tmp = df_list['Skyline_B00111.csv'].drop('City_State')\n",
    "tmp = tmp.withColumnRenamed(\"Street_Address\", \"PICK_UP_ADDRESS\")\n",
    "df_list['Skyline_B00111.csv'] = tmp\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(tmp)\n",
    "df_list['Skyline_B00111.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c895df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320641"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10.'Prestige_B01338.csv'\n",
    "FHV_10_companies_data = FHV_10_companies_data.union(df_list['Prestige_B01338.csv'])\n",
    "df_list['Prestige_B01338.csv'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a2db351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1409080"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FHV_10_companies_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70903dd7",
   "metadata": {},
   "source": [
    "## Read the unified \"FHV_10_companies_data\" file in parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9f5889e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/16 00:17:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: Date, Time, PU_Address\n",
      " Schema: Date, Time, PU_Address5\n",
      "Expected: PU_Address5 but found: PU_Address\n",
      "CSV file: file:///home/naeim/Dropbox/UU_Master/Fourth%20Semester/Data%20Engineering/Assignments/Project/Naeim_code/data_engineering1_project/DATA/Federal_02216.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if server_mode:\n",
    "    path = 'hdfs://192.168.2.74:9000/user/ubuntu/uber-tlc-foil-response/other-FHV-data'\n",
    "else:\n",
    "    path = os.getcwd() + '/DATA/'\n",
    "\n",
    "FHV_file = path+'FHV_10_companies_data.parquet'\n",
    "FHV_10_companies_data.write.mode(\"overwrite\").parquet(FHV_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f4663e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DATE: string (nullable = true)\n",
      " |-- TIME: string (nullable = true)\n",
      " |-- PICK_UP_ADDRESS: string (nullable = true)\n",
      "\n",
      "+--------+----+---------------+\n",
      "|    DATE|TIME|PICK_UP_ADDRESS|\n",
      "+--------+----+---------------+\n",
      "|7/1/2014|0:00|260 W 44 St NYC|\n",
      "|7/1/2014|0:00|125 W 29 St Nyc|\n",
      "|7/1/2014|0:00|141 W 28 St Nyc|\n",
      "+--------+----+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1409080"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FHV_10_Co = spark_session.read.parquet(FHV_file)\n",
    "FHV_10_Co.printSchema()\n",
    "FHV_10_Co.show(3)\n",
    "FHV_10_Co.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "25fcb190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark closed!\n"
     ]
    }
   ],
   "source": [
    "sc.stop()\n",
    "print(\"Spark closed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cae4fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
